{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLUE logs processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.5.0\n",
    "!pip install datasets==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, load_from_disk, load_dataset, inspect_metric\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AlbertTokenizer,\\\n",
    "T5Tokenizer, DebertaTokenizer, GPT2Tokenizer\n",
    "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification,\\\n",
    "RobertaForSequenceClassification, AlbertForSequenceClassification, T5ForConditionalGeneration,\\\n",
    "DebertaForSequenceClassification, GPT2ForSequenceClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving datasets and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task in GLUE_TASKS: \n",
    "    actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "    load_dataset(\"glue\", actual_task).save_to_disk(\"datasets/glue/\" + actual_task)\n",
    "    load_metric(\"glue\", actual_task)\n",
    "    inspect_metric(\"glue\", \"metrics/glue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"metrics/glue\", \"cola\")\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving tokenizers and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_to_model = {\n",
    "    \"distilbert-base-uncased\": {\n",
    "        \"model\": DistilBertForSequenceClassification,\n",
    "        \"tokenizer\": DistilBertTokenizer\n",
    "    },\n",
    "    \"bert-base-uncased\": {\n",
    "        \"model\": BertForSequenceClassification,\n",
    "        \"tokenizer\": BertTokenizer\n",
    "    },\n",
    "    \"bert-large-uncased\": {\n",
    "        \"model\": BertForSequenceClassification,\n",
    "        \"tokenizer\": BertTokenizer\n",
    "    },\n",
    "    \"roberta-base\": {\n",
    "        \"model\": RobertaForSequenceClassification,\n",
    "        \"tokenizer\": RobertaTokenizer\n",
    "    },\n",
    "    \"roberta-large\": {\n",
    "        \"model\": RobertaForSequenceClassification,\n",
    "        \"tokenizer\": RobertaTokenizer\n",
    "    },\n",
    "    \"distilroberta-base\": {\n",
    "        \"model\": RobertaForSequenceClassification,\n",
    "        \"tokenizer\": RobertaTokenizer\n",
    "    },\n",
    "    \"albert-base-v2\": {\n",
    "        \"model\": AlbertForSequenceClassification,\n",
    "        \"tokenizer\": AlbertTokenizer\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"model\": AlbertForSequenceClassification,\n",
    "        \"tokenizer\": AlbertTokenizer\n",
    "    },\n",
    "    \"t5-base\": {\n",
    "        \"model\": T5ForConditionalGeneration,\n",
    "        \"tokenizer\": T5Tokenizer\n",
    "    },\n",
    "    \"deberta-base\": {\n",
    "        \"model\": DebertaForSequenceClassification,\n",
    "        \"tokenizer\": DebertaTokenizer\n",
    "    },\n",
    "    \"deberta-large\": {\n",
    "        \"model\": DebertaForSequenceClassification,\n",
    "        \"tokenizer\": DebertaTokenizer\n",
    "    },\n",
    "    \"gpt2\": {\n",
    "        \"model\": GPT2ForSequenceClassification,\n",
    "        \"tokenizer\": GPT2Tokenizer\n",
    "    },\n",
    "    \"distilgpt2\": {\n",
    "        \"model\": GPT2ForSequenceClassification,\n",
    "        \"tokenizer\": GPT2Tokenizer\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in name_to_model.keys():\n",
    "    if model_name == 'deberta-base':\n",
    "        model_id = 'microsoft/deberta-base'\n",
    "    elif model_name == 'deberta-large':\n",
    "        model_id = 'microsoft/deberta-large'\n",
    "    else:\n",
    "        model_id = model_name\n",
    "    model_tokenizer = name_to_model[model_name][\"tokenizer\"]\n",
    "    model_tokenizer = model_tokenizer.from_pretrained(model_id)\n",
    "    model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "    model_tokenizer.save_pretrained(\"tokenizers/{}\".format(model_name))\n",
    "    ! mv tokenizers/{model_name}/tokenizer_config.json tokenizers/{model_name}/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in name_to_model.keys():\n",
    "    if model_name == 'deberta-base':\n",
    "        model_id = 'microsoft/deberta-base'\n",
    "    elif model_name == 'deberta-large':\n",
    "        model_id = 'microsoft/deberta-large'\n",
    "    else:\n",
    "        model_id = model_name\n",
    "    for num_labels in [1, 2, 3]:\n",
    "        model = name_to_model[model_name][\"model\"].from_pretrained(model_id, num_labels=num_labels)\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        model.save_pretrained(\"models/{}/{}\".format(model_name, num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import json\n",
    "\n",
    "results = {}\n",
    "model_name = ''\n",
    "seed = 0\n",
    "DIR = 'results'\n",
    "\n",
    "for log in listdir(DIR):\n",
    "    if log[-3:] == 'err':\n",
    "        continue\n",
    "    with open(\"{}/{}\".format(DIR, log)) as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.replace(\"'\", \"\\\"\"))\n",
    "            if \"task_name\" in line.keys():\n",
    "                task_name = line[\"task_name\"]\n",
    "                model_name = line[\"model_name\"]\n",
    "                results[model_name] = results.get(model_name, {})\n",
    "                seed = str(line[\"random_seed\"])\n",
    "                results[model_name][task_name] = results[model_name].get(task_name, {})\n",
    "            elif \"eval_loss\" in line.keys():\n",
    "                del line['epoch']\n",
    "                del line['eval_steps_per_second']\n",
    "                del line['eval_samples_per_second']\n",
    "                del line['eval_runtime']\n",
    "                del line['eval_loss']\n",
    "                for metric in line.keys():\n",
    "                    results[model_name][task_name][metric]       = results[model_name][task_name].get(metric, {})\n",
    "                    results[model_name][task_name][metric][seed] = results[model_name][task_name][metric].get(seed, [])\n",
    "                    results[model_name][task_name][metric][seed].append(line[metric])\n",
    "\n",
    "for model_name in results.keys():\n",
    "    for task in results[model_name].keys():\n",
    "        for metric in results[model_name][task].keys():\n",
    "            for seed in results[model_name][task][metric].keys():\n",
    "                results[model_name][task][metric][seed] = max(results[model_name][task][metric][seed]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_results = {a:{c:{e:round(np.mean(list(f.values())), 1) for e,f in d.items()} for c,d in b.items()} for a,b in results.items()}\n",
    "\n",
    "df = pd.DataFrame(columns=list(pd.io.json.json_normalize(list(mean_results.values())[0]).columns))\n",
    "\n",
    "models = name_to_model.keys()\n",
    "actual_models = []\n",
    "\n",
    "for model_name in models:\n",
    "    if model_name in mean_results.keys():\n",
    "        actual_models.append(model_name)\n",
    "        df = df.append(pd.io.json.json_normalize(mean_results[model_name]), sort=False)\n",
    "\n",
    "df['model_name'] = actual_models\n",
    "df = df.set_index('model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../tables/case_study_4/glue_hpc.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
